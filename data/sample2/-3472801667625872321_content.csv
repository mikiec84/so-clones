"Content"
"    #! /usr/bin/env python&#xD;&#xA;    # -*- coding: iso-8859-1 -*-&#xD;&#xA;    # vi:ts=4:et&#xD;&#xA;    # $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $&#xD;&#xA;    &#xD;&#xA;    #&#xD;&#xA;    # Usage: python retriever-multi.py <file with URLs to fetch> [<# of&#xD;&#xA;    #          concurrent connections>]&#xD;&#xA;    #&#xD;&#xA;    &#xD;&#xA;    import sys&#xD;&#xA;    import pycurl&#xD;&#xA;    &#xD;&#xA;    # We should ignore SIGPIPE when using pycurl.NOSIGNAL - see&#xD;&#xA;    # the libcurl tutorial for more info.&#xD;&#xA;    try:&#xD;&#xA;        import signal&#xD;&#xA;        from signal import SIGPIPE, SIG_IGN&#xD;&#xA;        signal.signal(signal.SIGPIPE, signal.SIG_IGN)&#xD;&#xA;    except ImportError:&#xD;&#xA;        pass&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    # Get args&#xD;&#xA;    num_conn = 10&#xD;&#xA;    try:&#xD;&#xA;        if sys.argv[1] == """"-"""":&#xD;&#xA;            urls = sys.stdin.readlines()&#xD;&#xA;        else:&#xD;&#xA;            urls = open(sys.argv[1]).readlines()&#xD;&#xA;        if len(sys.argv) >= 3:&#xD;&#xA;            num_conn = int(sys.argv[2])&#xD;&#xA;    except:&#xD;&#xA;        print """"Usage: %s <file with URLs to fetch> [<# of concurrent connections>]"""" % sys.argv[0]&#xD;&#xA;        raise SystemExit&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    # Make a queue with (url, filename) tuples&#xD;&#xA;    queue = []&#xD;&#xA;    for url in urls:&#xD;&#xA;        url = url.strip()&#xD;&#xA;        if not url or url[0] == """"#"""":&#xD;&#xA;            continue&#xD;&#xA;        filename = """"doc_%03d.dat"""" % (len(queue) + 1)&#xD;&#xA;        queue.append((url, filename))&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    # Check args&#xD;&#xA;    assert queue, """"no URLs given""""&#xD;&#xA;    num_urls = len(queue)&#xD;&#xA;    num_conn = min(num_conn, num_urls)&#xD;&#xA;    assert 1 <= num_conn <= 10000, """"invalid number of concurrent connections""""&#xD;&#xA;    print """"PycURL %s (compiled against 0x%x)"""" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)&#xD;&#xA;    print """"----- Getting"""", num_urls, """"URLs using"""", num_conn, """"connections -----""""&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    # Pre-allocate a list of curl objects&#xD;&#xA;    m = pycurl.CurlMulti()&#xD;&#xA;    m.handles = []&#xD;&#xA;    for i in range(num_conn):&#xD;&#xA;        c = pycurl.Curl()&#xD;&#xA;        c.fp = None&#xD;&#xA;        c.setopt(pycurl.FOLLOWLOCATION, 1)&#xD;&#xA;        c.setopt(pycurl.MAXREDIRS, 5)&#xD;&#xA;        c.setopt(pycurl.CONNECTTIMEOUT, 30)&#xD;&#xA;        c.setopt(pycurl.TIMEOUT, 300)&#xD;&#xA;        c.setopt(pycurl.NOSIGNAL, 1)&#xD;&#xA;        m.handles.append(c)&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    # Main loop&#xD;&#xA;    freelist = m.handles[:]&#xD;&#xA;    num_processed = 0&#xD;&#xA;    while num_processed < num_urls:&#xD;&#xA;        # If there is an url to process and a free curl object, add to multi stack&#xD;&#xA;        while queue and freelist:&#xD;&#xA;            url, filename = queue.pop(0)&#xD;&#xA;            c = freelist.pop()&#xD;&#xA;            c.fp = open(filename, """"wb"""")&#xD;&#xA;            c.setopt(pycurl.URL, url)&#xD;&#xA;            c.setopt(pycurl.WRITEDATA, c.fp)&#xD;&#xA;            m.add_handle(c)&#xD;&#xA;            # store some info&#xD;&#xA;            c.filename = filename&#xD;&#xA;            c.url = url&#xD;&#xA;        # Run the internal curl state machine for the multi stack&#xD;&#xA;        while 1:&#xD;&#xA;            ret, num_handles = m.perform()&#xD;&#xA;            if ret != pycurl.E_CALL_MULTI_PERFORM:&#xD;&#xA;                break&#xD;&#xA;        # Check for curl objects which have terminated, and add them to the freelist&#xD;&#xA;        while 1:&#xD;&#xA;            num_q, ok_list, err_list = m.info_read()&#xD;&#xA;            for c in ok_list:&#xD;&#xA;                c.fp.close()&#xD;&#xA;                c.fp = None&#xD;&#xA;                m.remove_handle(c)&#xD;&#xA;                print """"Success:"""", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)&#xD;&#xA;                freelist.append(c)&#xD;&#xA;            for c, errno, errmsg in err_list:&#xD;&#xA;                c.fp.close()&#xD;&#xA;                c.fp = None&#xD;&#xA;                m.remove_handle(c)&#xD;&#xA;                print """"Failed: """", c.filename, c.url, errno, errmsg&#xD;&#xA;                freelist.append(c)&#xD;&#xA;            num_processed = num_processed + len(ok_list) + len(err_list)&#xD;&#xA;            if num_q == 0:&#xD;&#xA;                break&#xD;&#xA;        # Currently no more I/O is pending, could do something in the meantime&#xD;&#xA;        # (display a progress bar, etc.).&#xD;&#xA;        # We just call select() to sleep until some more data is available.&#xD;&#xA;        m.select(1.0)&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    # Cleanup&#xD;&#xA;    for c in m.handles:&#xD;&#xA;        if c.fp is not None:&#xD;&#xA;            c.fp.close()&#xD;&#xA;            c.fp = None&#xD;&#xA;        c.close()&#xD;&#xA;    m.close()"
